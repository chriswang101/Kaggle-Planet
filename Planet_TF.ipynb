{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filepath of the dataset\n",
    "pre_filepath = \"../../../../../../Volumes/Seagate Backup Plus Drive/Documents/Kaggle Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.placeholder(tf.float32, [1, 64, 64, 3])\n",
    "y_train = tf.placeholder(tf.float32, [1, 17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramaters definitions\n",
    "VALIDATION_SPLIT = 35000\n",
    "BATCH_SIZE = 64\n",
    "n_classes = int(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_data(images, split):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        images: Numpy array of all the input images.\n",
    "        split: Number of input images to use for training. The rest will be used for validation.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of 4 numpy arrays with format (X_train, X_valid, y_train, y_valid)\n",
    "    \"\"\"\n",
    "    # Ensure validation split is valid\n",
    "    num_examples = images.shape[0]\n",
    "    if validation_split > num_examples:\n",
    "        raise Exception('validation_split greater than number of examples')\n",
    "    \n",
    "    # Split the training sets into training and validation sets\n",
    "    X_train, X_valid = X_train[:validation_split], X_train[validation_split:]\n",
    "    y_train, y_valid = y_train[:validation_split], y_train[validation_split:]\n",
    "    \n",
    "    return (X_train, X_valid, y_train, y_valid)\n",
    "    \n",
    "# Helper wrappers\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    return_val = tf.nn.conv2d(x, W, strides=[1,strides,strides,1], padding='SAME')\n",
    "    return_val = tf.nn.bias_add(return_val, b)\n",
    "    return tf.nn.relu(return_val)\n",
    "\n",
    "def maxpool(x, pool_size=2):\n",
    "    return tf.nn.max_pool(x, [1,pool_size,pool_size,1], [1,pool_size,pool_size,1], padding='SAME')\n",
    "\n",
    "def fc(x, W, b):\n",
    "    return_val = tf.matmul(x, W)\n",
    "    return_val = tf.nn.bias_add(return_val, b)\n",
    "    return tf.nn.relu(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Tensorflow model!\n",
    "def model(images, weights, biases, dropout=0.8):\n",
    "    \"\"\"\n",
    "    Defines the image classification model\n",
    "    \n",
    "    Inputs:\n",
    "        images: entire training set of images\n",
    "        input_shape: dimensions of input as a tuple\n",
    "    \n",
    "    Outputs logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply convolution and pooling to each layer\n",
    "    conv1 = conv2d(images, weights['conv1'], biases['conv1'])  \n",
    "    conv1 = maxpool(conv1)\n",
    "    \n",
    "    conv2 = conv2d(conv1, weights['conv2'], biases['conv2'])\n",
    "    conv2 = maxpool(conv2)\n",
    "    \n",
    "    conv3 = conv2d(conv2, weights['conv3'], biases['conv3'])\n",
    "    conv3 = maxpool(conv3)\n",
    "    \n",
    "    # Apply dropout\n",
    "    conv3 = tf.nn.dropout(conv3, dropout)\n",
    "    \n",
    "    # First reshape output of conv3 into a vector\n",
    "    conv3_vec = tf.reshape(conv3, [1, 8*8*128])\n",
    "    \n",
    "    # FC layers\n",
    "    fc1 = fc(conv3_vec, weights['fc1'], biases['fc1'])\n",
    "    # Then apply dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    fc2 = fc(fc1, weights['fc2'], biases['fc2'])\n",
    "    \n",
    "    # Return logits which is a vector of 17 class scores\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'conv1':tf.Variable(tf.random_normal([3,3,3,32])), # 3 by 3 convolution, 3 channels (depth), 32 outputs\n",
    "           'conv2':tf.Variable(tf.random_normal([3,3,32,64])), # 3 by 3 convolution, 32 inputs, 64 outputs\n",
    "           'conv3':tf.Variable(tf.random_normal([3,3,64,128])), # 3 by 3 convolution, 64 inputs, 128 outputs\n",
    "           'fc1':tf.Variable(tf.random_normal([8*8*128,1024])), \n",
    "           'fc2':tf.Variable(tf.random_normal([1024,n_classes]))}\n",
    "\n",
    "biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "          'conv2':tf.Variable(tf.random_normal([64])),\n",
    "          'conv3':tf.Variable(tf.random_normal([128])),\n",
    "          'fc1':tf.Variable(tf.random_normal([1024])),\n",
    "          'fc2':tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "conv_model = model(X_train, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load X_train and y_train arrays from disk\n",
    "# X_train = np.load(pre_filepath + \"/Planet/model-data/X_train_arr.npy\")\n",
    "# y_train = np.load(pre_filepath + \"/Planet/model-data/y_train_arr.npy\")\n",
    "\n",
    "# Load X_train and y_train arrays from disk\n",
    "X_train = np.load(\"model-data/X_train_arr.npy\")\n",
    "y_train = np.load(\"model-data/y_train_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-9693655e07e7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-9693655e07e7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    optimizer = tf.train.AdamOptimizer().minimize()\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_train,\n",
    "                                                              logits=conv_model)\n",
    "optimizer = tf.train.AdamOptimizer().minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
