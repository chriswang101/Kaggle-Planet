{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filepath of the dataset\n",
    "pre_filepath = \"../../../../../../Volumes/Seagate Backup Plus Drive/Documents/Kaggle Datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paramaters definitions\n",
    "VALIDATION_SPLIT = 35000\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 5\n",
    "n_batches = VALIDATION_SPLIT//BATCH_SIZE\n",
    "n_classes = int(y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_placeholder = tf.placeholder(tf.float32, [BATCH_SIZE, 64, 64, 3])\n",
    "y_train_placeholder = tf.placeholder(tf.uint8, [BATCH_SIZE, 17])\n",
    "X_train_batch = tf.train.batch([X_train], batch_size=BATCH_SIZE, capacity=BATCH_SIZE)\n",
    "y_train_batch = tf.train.batch([y_train], batch_size=BATCH_SIZE, capacity=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper wrappers\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    return_val = tf.nn.conv2d(x, W, strides=[1,strides,strides,1], padding='SAME')\n",
    "    return_val = tf.nn.bias_add(return_val, b)\n",
    "    return tf.nn.relu(return_val)\n",
    "\n",
    "def maxpool(x, pool_size=2):\n",
    "    return tf.nn.max_pool(x, [1,pool_size,pool_size,1], [1,pool_size,pool_size,1], padding='SAME')\n",
    "\n",
    "def fc(x, W, b):\n",
    "    return_val = tf.matmul(x, W)\n",
    "    return_val = tf.nn.bias_add(return_val, b)\n",
    "    return tf.nn.relu(return_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the Tensorflow model!\n",
    "def model(images, weights, biases, dropout=0.8):\n",
    "    \"\"\"\n",
    "    Defines the image classification model\n",
    "    \n",
    "    Inputs:\n",
    "        images: entire training set of images\n",
    "        input_shape: dimensions of input as a tuple\n",
    "    \n",
    "    Outputs logits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply convolution and pooling to each layer\n",
    "    conv1 = conv2d(images, weights['conv1'], biases['conv1'])  \n",
    "    conv1 = maxpool(conv1)\n",
    "    \n",
    "    conv2 = conv2d(conv1, weights['conv2'], biases['conv2'])\n",
    "    conv2 = maxpool(conv2)\n",
    "    \n",
    "    conv3 = conv2d(conv2, weights['conv3'], biases['conv3'])\n",
    "    conv3 = maxpool(conv3)\n",
    "    \n",
    "    # Apply dropout\n",
    "    conv3 = tf.nn.dropout(conv3, dropout)\n",
    "    \n",
    "    # First reshape output of conv3 into a vector\n",
    "    conv3_vec = tf.reshape(conv3, [1, 8*8*128])\n",
    "    \n",
    "    # FC layers\n",
    "    fc1 = fc(conv3_vec, weights['fc1'], biases['fc1'])\n",
    "    # Then apply dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    fc2 = fc(fc1, weights['fc2'], biases['fc2'])\n",
    "    \n",
    "    # Return logits which is a vector of 17 class scores\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {'conv1':tf.Variable(tf.random_normal([3,3,3,32])), # 3 by 3 convolution, 3 channels (depth), 32 outputs\n",
    "           'conv2':tf.Variable(tf.random_normal([3,3,32,64])), # 3 by 3 convolution, 32 inputs, 64 outputs\n",
    "           'conv3':tf.Variable(tf.random_normal([3,3,64,128])), # 3 by 3 convolution, 64 inputs, 128 outputs\n",
    "           'fc1':tf.Variable(tf.random_normal([8*8*128,1024])), \n",
    "           'fc2':tf.Variable(tf.random_normal([1024,n_classes]))}\n",
    "\n",
    "biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "          'conv2':tf.Variable(tf.random_normal([64])),\n",
    "          'conv3':tf.Variable(tf.random_normal([128])),\n",
    "          'fc1':tf.Variable(tf.random_normal([1024])),\n",
    "          'fc2':tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "pred_logits = model(X_train, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_train,\n",
    "                                                              logits=pred_logits))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# Get the prediction from the highest valued logit\n",
    "prediction = tf.argmax(pred_logits)\n",
    "prediction = tf.cast(prediction, tf.uint8)\n",
    "\n",
    "# Compute array of bools that indicate whether the prediction was correct\n",
    "pred_correct = tf.equal(y_train, prediction)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load X_train and y_train arrays from disk\n",
    "# X_train = np.load(pre_filepath + \"/Planet/model-data/X_train_arr.npy\")\n",
    "# y_train = np.load(pre_filepath + \"/Planet/model-data/y_train_arr.npy\")\n",
    "\n",
    "# Load X_train and y_train arrays from disk\n",
    "X_train_arr = np.load(\"model-data/X_train_arr.npy\")\n",
    "y_train_arr = np.load(\"model-data/y_train_arr.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting up the training data into training and validation sets\n",
    "X_train_arr = X_train_arr[:VALIDATION_SPLIT]\n",
    "y_train_arr = y_train_arr[:VALIDATION_SPLIT]\n",
    "\n",
    "X_valid_arr = X_train_arr[VALIDATION_SPLIT:]\n",
    "y_valid_arr = y_train_arr[VALIDATION_SPLIT:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-24831a7b48ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mfeed_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train_img\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(EPOCHS):\n",
    "        training_loss = 0\n",
    "        train_accuracies = np.array([])\n",
    "        for image in range(VALIDATION_SPLIT):\n",
    "            # Extract the next BATCH_SIZE images and labels\n",
    "            X_train_img = X_train_arr[image]\n",
    "            y_train_img = y_train_arr[image]\n",
    "            \n",
    "            # Run the model\n",
    "            feed_dict_train = {X_train:X_train_img, y_train:y_train_img}\n",
    "            loss, accuracy = sess.run([loss, accuracy], feed_dict=feed_dict_train)\n",
    "            np.append(train_accuracies, accuracy)\n",
    "            \n",
    "            training_loss += loss\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        train_accuracy = np.mean(train_accuracies)\n",
    "        \n",
    "        # Compute validation accuracy\n",
    "        feed_dict_valid = {X_train_placeholder:X_valid_arr, y_train_placeholder:y_valid_arr}\n",
    "        valid_loss, valid_accuracy = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "        print(\"Epoch: \" + epoch)\n",
    "        print(\"Training loss: \" + training_loss + \"Training accuracy: \" + train_accuracy)\n",
    "        print(\"Validation loss: \" + valid_loss + \"Validation accuracy: \" + valid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr[1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
